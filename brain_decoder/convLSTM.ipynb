{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('/home/seigyo/Documents/pytorch/brain_decoder')\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import mne\n",
    "from mne.io import concatenate_raws\n",
    "from mymodule.utils import data_loader, evaluator\n",
    "from mymodule.layers import LSTM, Residual_block, Res_net, Wavelet_cnn, NlayersSeqConvLSTM\n",
    "from mymodule.trainer import Trainer\n",
    "from mymodule.optim import Eve, YFOptimizer\n",
    "from sklearn.utils import shuffle\n",
    "from tensorboardX import SummaryWriter\n",
    "from load_data import get_data, get_data_multi, get_crops, get_crops_multi\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 10\n",
    "cv_splits = 5\n",
    "torch.manual_seed(1214)\n",
    "torch.cuda.manual_seed_all(1214)\n",
    "num_of_subjects = 30\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(id=1, event_code=[6,10,14], filter=[0.5, 36], t=[1, 4.1]):\n",
    "    # 5,6,7,10,13,14 are codes for executed and imagined hands/feet\n",
    "    subject_id = id\n",
    "    event_codes = event_code\n",
    "\n",
    "    # This will download the files if you don't have them yet,\n",
    "    # and then return the paths to the files.\n",
    "    physionet_paths = mne.datasets.eegbci.load_data(subject_id, event_codes)\n",
    "\n",
    "    # Load each of the files\n",
    "    parts = [mne.io.read_raw_edf(path, preload=True,stim_channel='auto', verbose='WARNING')\n",
    "             for path in physionet_paths]\n",
    "\n",
    "    # Concatenate them\n",
    "    raw = concatenate_raws(parts)\n",
    "\n",
    "    # bandpass filter\n",
    "    if filter != None:\n",
    "        raw.filter(filter[0], filter[1], fir_design='firwin', skip_by_annotation='edge')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Find the events in this dataset\n",
    "    events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "    # Use only EEG channels\n",
    "    eeg_channel_inds = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                       exclude='bads')\n",
    "\n",
    "    # Extract trials, only using EEG channels\n",
    "    epoched = mne.Epochs(raw, events, dict(hands=2, feet=3), tmin=t[0], tmax=t[1], proj=False, picks=eeg_channel_inds,\n",
    "                    baseline=None, preload=True)\n",
    "    # change time length\n",
    "    # epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "\n",
    "\n",
    "    # Convert data from volt to millivolt\n",
    "    # Pytorch expects float32 for input and int64 for labels.\n",
    "    X = (epoched.get_data() * 1e6).astype(np.float32)\n",
    "    y = (epoched.events[:,2] - 2).astype(np.int64) #2,3 -> 0,1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing orphaned offset at the beginning of the file.\n",
      "89 events found\n",
      "Events id: [1 2 3]\n",
      "45 matching events found\n",
      "Not setting metadata\n",
      "Loading data for 45 events and 497 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45, 64, 497)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_data()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 497, 1, 5, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_spat = np.zeros((45, X.shape[-1], 1, 5, 7))\n",
    "X_spat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spat[:, :, 0, 1, 0] = X[:, 0, :]\n",
    "X_spat[:, :, 0, 1, 1] = X[:, 1, :]\n",
    "X_spat[:, :, 0, 1, 2] = X[:, 2, :]\n",
    "X_spat[:, :, 0, 1, 3] = X[:, 3, :]\n",
    "X_spat[:, :, 0, 1, 4] = X[:, 4, :]\n",
    "X_spat[:, :, 0, 1, 5] = X[:, 5, :]\n",
    "X_spat[:, :, 0, 1, 6] = X[:, 6, :]\n",
    "\n",
    "X_spat[:, :, 0, 2, 0] = X[:, 7, :]\n",
    "X_spat[:, :, 0, 2, 1] = X[:, 8, :]\n",
    "X_spat[:, :, 0, 2, 2] = X[:, 9, :]\n",
    "X_spat[:, :, 0, 2, 3] = X[:, 10, :]\n",
    "X_spat[:, :, 0, 2, 4] = X[:, 11, :]\n",
    "X_spat[:, :, 0, 2, 5] = X[:, 12, :]\n",
    "X_spat[:, :, 0, 2, 6] = X[:, 13, :]\n",
    "\n",
    "X_spat[:, :, 0, 3, 0] = X[:, 14, :]\n",
    "X_spat[:, :, 0, 3, 1] = X[:, 15, :]\n",
    "X_spat[:, :, 0, 3, 2] = X[:, 16, :]\n",
    "X_spat[:, :, 0, 3, 3] = X[:, 17, :]\n",
    "X_spat[:, :, 0, 3, 4] = X[:, 18, :]\n",
    "X_spat[:, :, 0, 3, 5] = X[:, 19, :]\n",
    "X_spat[:, :, 0, 3, 6] = X[:, 20, :]\n",
    "\n",
    "X_spat[:, :, 0, 0, 0] = X[:, 30, :]\n",
    "X_spat[:, :, 0, 0, 1] = X[:, 31, :]\n",
    "X_spat[:, :, 0, 0, 2] = X[:, 32, :]\n",
    "X_spat[:, :, 0, 0, 3] = X[:, 33, :]\n",
    "X_spat[:, :, 0, 0, 4] = X[:, 34, :]\n",
    "X_spat[:, :, 0, 0, 5] = X[:, 35, :]\n",
    "X_spat[:, :, 0, 0, 6] = X[:, 36, :]\n",
    "\n",
    "X_spat[:, :, 0, 4, 0] = X[:, 47, :]\n",
    "X_spat[:, :, 0, 4, 1] = X[:, 48, :]\n",
    "X_spat[:, :, 0, 4, 2] = X[:, 49, :]\n",
    "X_spat[:, :, 0, 4, 3] = X[:, 50, :]\n",
    "X_spat[:, :, 0, 4, 4] = X[:, 51, :]\n",
    "X_spat[:, :, 0, 4, 5] = X[:, 52, :]\n",
    "X_spat[:, :, 0, 4, 6] = X[:, 53, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ConvLSTM, self).__init__()\n",
    "    self.relu = nn.LeakyReLU()\n",
    "    self.convlstm = NlayersSeqConvLSTM(input_channels=1,\n",
    "                        hidden_channels=[32, 64],\n",
    "                        kernel_sizes=[3,3])\n",
    "    self.conv = nn.Sequential(nn.Conv2d(64,128,(3,3)),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm2d(128),\n",
    "                      nn.Conv2d(128,256,(3,5)),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm2d(256)).cuda()\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.linear = nn.Linear(256, 2)\n",
    "    \n",
    "    \n",
    "  def forward(self, x):\n",
    "    h, _ = self.convlstm(x)\n",
    "    h = self.conv(h[:,-1,:,:,:])\n",
    "    h = h.view(-1, h.size(1))\n",
    "    h = self.dropout(h)\n",
    "    h = self.linear(h)\n",
    "    return h\n",
    "\n",
    "model = ConvLSTM().cuda()\n",
    "# model = NlayersSeqConvLSTM(input_channels=1,\n",
    "#                     hidden_channels=[32, 64],\n",
    "#                     kernel_sizes=[3,3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y, outputs = model(Variable(torch.from_numpy(X_spat)).float().cuda())\n",
    "# y[:,-1,:,:,:].size()\n",
    "\n",
    "y = model(Variable(torch.from_numpy(X_spat)).float().cuda())\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(nn.Conv2d(64,128,(3,3)),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm2d(128),\n",
    "                      nn.Conv2d(128,256,(3,5)),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm2d(256)).cuda()\n",
    "\n",
    "o = model2(y[:,-1,:,:,:])\n",
    "o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing orphaned offset at the beginning of the file.\n",
      "89 events found\n",
      "Events id: [1 2 3]\n",
      "45 matching events found\n",
      "Not setting metadata\n",
      "Loading data for 45 events and 641 original time points ...\n",
      "0 bad epochs dropped\n",
      "----------start training----------\n",
      "epoch:1, tr_loss:0.6228, tr_acc:0.5278,   val_loss:0.3525, val_acc:0.3333\n",
      "epoch:2, tr_loss:0.4660, tr_acc:0.6944,   val_loss:0.3527, val_acc:0.3333\n",
      "epoch:3, tr_loss:0.3752, tr_acc:0.8333,   val_loss:0.3528, val_acc:0.3333\n",
      "epoch:4, tr_loss:0.3534, tr_acc:0.9167,   val_loss:0.3531, val_acc:0.3333\n",
      "epoch:5, tr_loss:0.3693, tr_acc:0.8889,   val_loss:0.3525, val_acc:0.3333\n",
      "epoch:6, tr_loss:0.3499, tr_acc:0.8889,   val_loss:0.3512, val_acc:0.3333\n",
      "epoch:7, tr_loss:0.2827, tr_acc:0.9722,   val_loss:0.3483, val_acc:0.3333\n",
      "epoch:8, tr_loss:0.2576, tr_acc:0.9722,   val_loss:0.3467, val_acc:0.4444\n",
      "epoch:9, tr_loss:0.2470, tr_acc:0.9444,   val_loss:0.3445, val_acc:0.6667\n",
      "epoch:10, tr_loss:0.2555, tr_acc:0.9444,   val_loss:0.3434, val_acc:0.6667\n",
      "epoch:11, tr_loss:0.1963, tr_acc:0.9444,   val_loss:0.3392, val_acc:0.6667\n",
      "epoch:12, tr_loss:0.1637, tr_acc:1.0000,   val_loss:0.3396, val_acc:0.5556\n",
      "epoch:13, tr_loss:0.2314, tr_acc:0.9444,   val_loss:0.3425, val_acc:0.5556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b80303b62d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     acc = cv_train(ConvLSTM, torch.nn.CrossEntropyLoss,\n\u001b[1;32m     37\u001b[0m                    \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_spat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                    num_of_cv=cv_splits, batch_size=batch_size)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b80303b62d99>\u001b[0m in \u001b[0;36mcv_train\u001b[0;34m(model_class, criterion_class, optimizer_class, X, y, epoch, num_of_cv, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m                   \u001b[0mval_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                   writer=writer, gpu=True)\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_best_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pytorch/mymodule/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----------start training----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----------finish training---------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     print('training_best_acc:{}, val_best_acc:{}'.format(\n",
      "\u001b[0;32m~/Documents/pytorch/mymodule/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_batch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_of_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_batch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mn_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def cv_train(model_class, criterion_class, optimizer_class, X, y,\n",
    "             epoch=100, num_of_cv=10, batch_size=16):\n",
    "    kf = KFold(n_splits=num_of_cv, shuffle=True)\n",
    "    accuracy = []\n",
    "    for train_idx, val_idx in kf.split(X=X, y=y):\n",
    "        train_x, val_x = X[train_idx], X[val_idx]\n",
    "        train_y, val_y = y[train_idx], y[val_idx]\n",
    "        train_loader = data_loader(train_x, train_y, batch_size=batch_size,\n",
    "                           shuffle=True, gpu=False)\n",
    "        val_loader = data_loader(val_x, val_y, batch_size=batch_size)\n",
    "        writer = SummaryWriter()\n",
    "        model = model_class().cuda()\n",
    "        criterion = criterion_class()\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-4)\n",
    "        trainer = Trainer(model, criterion, optimizer,\n",
    "                  train_loader, val_loader,\n",
    "                  val_num=1, early_stopping=2,\n",
    "                  writer=writer, gpu=True)\n",
    "        trainer.run(epochs=epoch)\n",
    "        accuracy.append(trainer.val_best_acc)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "all_accs_list = []\n",
    "all_mean_list = []\n",
    "all_var_list = []\n",
    "\n",
    "for idx in range(num_of_subjects):\n",
    "    X, y = get_data(id=idx+1, event_code=[6,10,14], filter=[0.5, 30], t=[0., 4])\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2]).transpose(0,1,3,2)\n",
    "\n",
    "#     model = Conv_lstm()\n",
    "#     model.cuda()\n",
    "\n",
    "    acc = cv_train(ConvLSTM, torch.nn.CrossEntropyLoss,\n",
    "                   torch.optim.Adam, X_spat, y, epoch=epochs,\n",
    "                   num_of_cv=cv_splits, batch_size=batch_size)\n",
    "\n",
    "    mean = np.mean(acc)\n",
    "    var = np.var(acc)\n",
    "    print('subject{}   mean_acc:{}, var_acc:{}'.format(idx+1, mean, var))\n",
    "\n",
    "    all_accs_list.append(acc)\n",
    "    all_mean_list.append(mean)\n",
    "    all_var_list.append(var)\n",
    "\n",
    "all_mean = np.mean(all_accs_list)\n",
    "all_var = np.var(all_accs_list)\n",
    "\n",
    "print('all subjects  mean_acc:{}, var_acc:{}'.format(all_mean, all_var))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
