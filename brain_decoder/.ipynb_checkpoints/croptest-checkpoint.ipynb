{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 64, 640)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(60, 64, 160*4)\n",
    "y = np.random.randint(0,5,60)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 4 3 4 3 4 4 1 3 3 0 3 1 3 2 4 3 0 0 4 4 4 4 0 4 4 2 2 4 3 1 3 1 1 2 1\n",
      " 4 4 4 2 2 2 2 4 4 1 1 1 1 4 4 3 4 1 0 3 4 0 3]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 4.  4.  4. ...,  4.  4.  4.]\n",
      " ..., \n",
      " [ 4.  4.  4. ...,  4.  4.  4.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 3.  3.  3. ...,  3.  3.  3.]]\n",
      "[ 0.  0.  0. ...,  3.  3.  3.]\n",
      "(38400,)\n"
     ]
    }
   ],
   "source": [
    "y_array = np.float32([y]*160*4)\n",
    "print(y)\n",
    "print(y_array.T)\n",
    "print(y_array.T.reshape(-1))\n",
    "print(y_array.T.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 640, 64)\n"
     ]
    }
   ],
   "source": [
    "X_ = X.transpose(0,2,1)\n",
    "print(X_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 3)\n",
      "[[[1 1 1]\n",
      "  [0 0 0]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [0 0 0]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]]\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [2 2 2]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[[1,1,1],[0,0,0],[2,2,2],[3,3,3]],[[1,1,1],[0,0,0],[2,2,2],[3,3,3]]])\n",
    "print(test.shape)\n",
    "print(test)\n",
    "print(test.reshape(-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trial2crop(X, y, seq_len=160):\n",
    "    X_ = X.transpose(0,2,1).reshape(-1, X.shape[1])\n",
    "    y_ = np.float32([y]*X.shape[2]).T.reshape(-1)\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17381572, -0.41681894, -0.3424273 , ...,  2.02084888,\n",
       "        -0.49295002,  0.63921098],\n",
       "       [-0.05015315,  0.68267869, -1.69972208, ..., -0.23096809,\n",
       "        -0.344557  , -0.13653685],\n",
       "       [-1.07656261,  0.0052268 , -1.79977294, ..., -1.34403256,\n",
       "        -1.90850593, -1.87503042],\n",
       "       ..., \n",
       "       [ 0.88771167, -0.7537184 , -0.01471577, ..., -0.10046294,\n",
       "         0.82760399,  0.53470509],\n",
       "       [-1.27091361,  1.16586691,  0.3956533 , ..., -0.26141981,\n",
       "         0.86836294,  1.36104847],\n",
       "       [-0.5390205 ,  1.68463596,  0.07267454, ...,  0.45829225,\n",
       "         0.60054359, -0.24846644]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_, y_ = trial2crop(X, y)\n",
    "X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import mne\n",
    "from mne.io import concatenate_raws\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "\n",
    "def get_data(id=1, event_code=[5,6,9,10,13,14], filter=[0.5, 36], t=[1, 4.1]):\n",
    "    # 5,6,7,10,13,14 are codes for executed and imagined hands/feet\n",
    "    subject_id = id\n",
    "    event_codes = event_code\n",
    "\n",
    "    # This will download the files if you don't have them yet,\n",
    "    # and then return the paths to the files.\n",
    "    physionet_paths = mne.datasets.eegbci.load_data(subject_id, event_codes)\n",
    "\n",
    "    # Load each of the files\n",
    "    parts = [mne.io.read_raw_edf(path, preload=True,stim_channel='auto', verbose='WARNING')\n",
    "             for path in physionet_paths]\n",
    "\n",
    "    # Concatenate them\n",
    "    raw = concatenate_raws(parts)\n",
    "\n",
    "    # bandpass filter\n",
    "    if filter != None:\n",
    "        raw.filter(filter[0], filter[1], fir_design='firwin', skip_by_annotation='edge')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Find the events in this dataset\n",
    "    events = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "    # Use only EEG channels\n",
    "    eeg_channel_inds = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                       exclude='bads')\n",
    "\n",
    "    # Extract trials, only using EEG channels\n",
    "    epoched = mne.Epochs(raw, events, dict(hands=2, feet=3), tmin=t[0], tmax=t[1], proj=False, picks=eeg_channel_inds,\n",
    "                    baseline=None, preload=True)\n",
    "    # change time length\n",
    "    # epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "\n",
    "\n",
    "    # Convert data from volt to millivolt\n",
    "    # Pytorch expects float32 for input and int64 for labels.\n",
    "    X = (epoched.get_data() * 1e6).astype(np.float32)\n",
    "    y = (epoched.events[:,2] - 2).astype(np.int64) #2,3 -> 0,1\n",
    "    return X, y, epoched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Events id: [1 2 3]\n",
      "90 matching events found\n",
      "Not setting metadata\n",
      "Loading data for 90 events and 641 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "_, _, epochs = get_data(t=[0,4.0])\n",
    "epochs_list = []\n",
    "time_window = 1.0\n",
    "for i in range(160 * 3):\n",
    "    epochs_train = epochs.copy().crop(tmin=i * time_window/160, tmax=i * time_window/160 + time_window)\n",
    "    epochs_list.append(epochs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "label_list = []\n",
    "for epoch in epochs_list:\n",
    "    X = (epoch.get_data() * 1e6).astype(np.float32)\n",
    "    y = (epoch.events[:,2] - 2).astype(np.int64)\n",
    "    data_list.append(X)\n",
    "    label_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 90, 64, 161)\n",
      "(480, 90)\n"
     ]
    }
   ],
   "source": [
    "data_array = np.array(data_list)\n",
    "label_array = np.array(label_list)\n",
    "print(data_array.shape)\n",
    "print(label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43200, 64, 161)\n",
      "(43200,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array2 = data_array.reshape(-1, data_array.shape[-2], data_array.shape[-1])\n",
    "label_array2 = label_array.reshape(-1)\n",
    "print(data_array2.shape)\n",
    "print(label_array2.shape)\n",
    "label_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('/home/seigyo/Documents/pytorch/brain_decoder')\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import mne\n",
    "from mne.io import concatenate_raws\n",
    "from mymodule.utils import data_loader, evaluator\n",
    "from mymodule.layers import LSTM, Residual_block, Res_net, Wavelet_cnn, NlayersSeqConvLSTM\n",
    "from mymodule.trainer import Trainer\n",
    "from mymodule.optim import Eve, YFOptimizer\n",
    "from sklearn.utils import shuffle\n",
    "from tensorboardX import SummaryWriter\n",
    "from load_data import get_data, get_data_multi, get_crops, get_crops_multi\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 16\n",
    "cv_splits = 5\n",
    "torch.manual_seed(1214)\n",
    "torch.cuda.manual_seed_all(1214)\n",
    "num_of_subjects = 30\n",
    "\n",
    "\n",
    "# \n",
    "# class Conv_lstm(nn.Module):\n",
    "#   def __init__(self):\n",
    "#     super(Conv_lstm, self).__init__()\n",
    "#     self.conv_time = nn.Conv2d(1, 40, (25, 1))\n",
    "#     self.batchnorm1 = nn.BatchNorm2d(40)\n",
    "#     self.conv_spat = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "#     self.batchnorm2 = nn.BatchNorm2d(40)\n",
    "#     self.pool = nn.AvgPool2d(kernel_size=(75, 1), stride=(15, 1))\n",
    "#     self.dropout = nn.Dropout2d(p=0.5)\n",
    "#     self.lstm = LSTM(40, 10, batch_size, bidirectional=False,\n",
    "#                      gpu=True, return_seq=False)\n",
    "#     self.dropout_linear = nn.Dropout(p=0.5)\n",
    "#     self.classifier = nn.Linear(10, 2)\n",
    "# \n",
    "#   def forward(self, x):\n",
    "#     h = self.conv_time(x)\n",
    "#     h = self.batchnorm1(h)\n",
    "#     h = self.conv_spat(h)\n",
    "#     h = self.batchnorm2(h)\n",
    "#     h = self.pool(h)\n",
    "#     h = self.dropout(h)\n",
    "#     h = h.squeeze().transpose(1, 2)\n",
    "#     h = self.lstm(h)\n",
    "#     h = self.dropout_linear(h)\n",
    "#     h = self.classifier(h)\n",
    "#     return h\n",
    "\n",
    "class DeepFBCSP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeepFBCSP, self).__init__()\n",
    "    self.conv_time = nn.Conv2d(1, 40, (25, 1))\n",
    "    self.batchnorm1 = nn.BatchNorm2d(40)\n",
    "    self.conv_spat1 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.conv_spat2 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.conv_spat3 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.conv_spat4 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.conv_spat5 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.conv_spat6 = nn.Conv2d(40, 40, (1, 64), bias=False)\n",
    "    self.batchnorm21 = nn.BatchNorm2d(40)\n",
    "    self.batchnorm22 = nn.BatchNorm2d(40)\n",
    "    self.batchnorm23 = nn.BatchNorm2d(40)\n",
    "    self.batchnorm24 = nn.BatchNorm2d(40)\n",
    "    self.batchnorm25 = nn.BatchNorm2d(40)\n",
    "    self.batchnorm26 = nn.BatchNorm2d(40)\n",
    "    self.pool = nn.AvgPool2d(kernel_size=(75, 1), stride=(15, 1))\n",
    "    self.dropout = nn.Dropout2d(p=0.5)\n",
    "    self.score_ave = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    \n",
    "    self.time_freq_conv = nn.Sequential(nn.Conv2d(6, 100, (12, 5)),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout2d(p=0.5),\n",
    "                                     nn.Conv2d(100, 200, (12, 5)),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout2d(p=0.5),\n",
    "                                     nn.Conv2d(200, 300, (12, 5)),\n",
    "                                     nn.ReLU())\n",
    "    self.dropout_linear = nn.Dropout(p=0.5)\n",
    "    self.classifier = nn.Linear(300, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self.conv_time(x)\n",
    "    h = self.batchnorm1(h)\n",
    "    h1 = self.conv_spat1(h)\n",
    "    h1 = self.batchnorm21(h1)\n",
    "    h2 = self.conv_spat2(h)\n",
    "    h2 = self.batchnorm22(h2)\n",
    "    h3 = self.conv_spat3(h)\n",
    "    h3 = self.batchnorm23(h3)\n",
    "    h4 = self.conv_spat4(h)\n",
    "    h4 = self.batchnorm24(h4)\n",
    "    h5 = self.conv_spat5(h)\n",
    "    h5 = self.batchnorm25(h5)\n",
    "    h6 = self.conv_spat6(h)\n",
    "    h6 = self.batchnorm26(h6)\n",
    "    h = torch.cat([h1,h2,h3,h4,h5,h6],dim=3)\n",
    "    h = h.transpose(1,-1)\n",
    "    h = self.time_freq_conv(h)\n",
    "    h = self.score_ave(h).view(-1,300)\n",
    "    h = self.dropout(h)\n",
    "    h = self.classifier(h)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepFBCSP().cuda()\n",
    "x = Variable(torch.randn((32,1,160,64))).cuda()\n",
    "y = model(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
